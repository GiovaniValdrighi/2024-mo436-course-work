{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook explores evaluates how a DeepLabV3 model performs on seismic segmentation task with few data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  1. Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1716473739.058885] [dac6c281a48b:949842:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Run unity tests\n",
    "run_unity_test = True\n",
    "\n",
    "# Setup trainer\n",
    "max_epochs=2\n",
    "\n",
    "# Pretrained weights filename\n",
    "pretrained_weights_filename = f'backbone_parameters-epochs-50.pth'\n",
    "#pretrained_weights_filename = None\n",
    "\n",
    "# Check if file exists\n",
    "if pretrained_weights_filename and not os.path.exists(pretrained_weights_filename):\n",
    "    raise Exception(f\"Could not find file {pretrained_weights_filename}. \"+\n",
    "                    \"Please set the pretrained_weights_filename variable with a valid filename.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Auxiliary functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the downstream model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape =  torch.Size([2, 6, 255, 701])\n"
     ]
    }
   ],
   "source": [
    "import models.deeplabv3 as dlv3\n",
    "\n",
    "# Build the downstream model\n",
    "def build_downstream_model(backbone_weights_filepath=None):\n",
    "    # Build and load the backbone weights\n",
    "    backbone = dlv3.DeepLabV3Backbone()\n",
    "    backbone.load_state_dict(torch.load(backbone_weights_filepath))\n",
    "\n",
    "    # Build the downstream model\n",
    "    downstream_model = dlv3.DeepLabV3Model(backbone = backbone, num_classes=6)\n",
    "    downstream_model.to(device=device)\n",
    "\n",
    "    return downstream_model\n",
    "\n",
    "if run_unity_test:\n",
    "    # Test the model\n",
    "    downstream_model = build_downstream_model(pretrained_weights_filename)\n",
    "    random_input = torch.rand(2,3,255,701).to(device=device)\n",
    "    output = downstream_model(random_input)\n",
    "    print(\"output_shape = \", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "def evaluate_model(model, dataset_dl):\n",
    "    # Inicialize JaccardIndex metric\n",
    "    jaccard = JaccardIndex(task=\"multiclass\", num_classes=6)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # For each batch, compute the predictions and compare with the labels.\n",
    "    for X, y in dataset_dl:\n",
    "        # Move the model, data and metric to the GPU if available\n",
    "        model.to(device)\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        jaccard.to(device)\n",
    "\n",
    "        logits = model(X.float())\n",
    "        predictions = torch.argmax(logits, dim=1, keepdim=True)\n",
    "        jaccard(predictions, y)\n",
    "    # Return a tuple with the number of correct predictions and the total number of predictions\n",
    "    return (float(jaccard.compute().to(\"cpu\")))\n",
    "\n",
    "def report_accuracy(model, dataset_dl, prefix=\"\"):\n",
    "    iou = evaluate_model(model, dataset_dl)\n",
    "    print(prefix + \" IoU = {:0.4f}\".format(iou))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train using multiple subsets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate SeismicDataModule\n",
    "from data_modules.seismic import F3SeismicDataModule\n",
    "\n",
    "# Instantiating the SeismicDataModule with root dir at data/f3\n",
    "data_module = F3SeismicDataModule(\"data/\", batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data loader with 1.00% of data\n",
      "Generating data loader with 5.00% of data\n",
      "Generating data loader with 10.00% of data\n",
      "Generating data loader with 20.00% of data\n",
      "Generating data loader with 50.00% of data\n",
      "Generating data loader with 100.00% of data\n",
      "6 dataloaders were defined:\n",
      " - Train/Val dataloader with cap=0.01 has 1/1 batch(es) and 9/1 sample(s)!\n",
      " - Train/Val dataloader with cap=0.05 has 6/1 batch(es) and 49/5 sample(s)!\n",
      " - Train/Val dataloader with cap=0.1 has 12/2 batch(es) and 99/11 sample(s)!\n",
      " - Train/Val dataloader with cap=0.2 has 24/3 batch(es) and 198/22 sample(s)!\n",
      " - Train/Val dataloader with cap=0.5 has 62/7 batch(es) and 496/55 sample(s)!\n",
      " - Train/Val dataloader with cap=1.0 has 124/14 batch(es) and 992/110 sample(s)!\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary that maps the cap ratio to each dataloader.\n",
    "dataloaders = {}\n",
    "for cap in [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]:\n",
    "    print(\"Generating data loader with {0:.2f}% of data\".format(cap*100) )\n",
    "    dataloaders[cap] = {\n",
    "        \"train_dl\": data_module.train_dataloader(cap=cap, drop_last=True),\n",
    "        \"val_dl\": data_module.val_dataloader(cap=cap),\n",
    "    }\n",
    "\n",
    "if run_unity_test:\n",
    "    print(f\"{len(dataloaders)} dataloaders were defined:\")\n",
    "    for cap, d in dataloaders.items():\n",
    "        train_dl = d[\"train_dl\"]\n",
    "        val_dl = d[\"val_dl\"]\n",
    "        print(f\" - Train/Val dataloader with cap={cap} has {len(train_dl)}/{len(val_dl)} batch(es) and {len(train_dl.dataset)}/{len(val_dl.dataset)} sample(s)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model with cap=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                    | Params\n",
      "------------------------------------------------------\n",
      "0 | backbone  | DeepLabV3Backbone       | 25.6 M\n",
      "1 | pred_head | DeepLabV3PredictionHead | 16.1 M\n",
      "2 | loss_fn   | CrossEntropyLoss        | 0     \n",
      "------------------------------------------------------\n",
      "41.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "41.7 M    Total params\n",
      "166.736   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s, v_num=113]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1/1 [00:01<00:00,  0.83it/s, v_num=113]\n",
      "Best model was saved at: /workspaces/2024-mo436-course-work/lightning_logs/version_113/checkpoints/epoch=1-step=2.ckpt\n",
      "Evaluating model with cap=0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                    | Params\n",
      "------------------------------------------------------\n",
      "0 | backbone  | DeepLabV3Backbone       | 25.6 M\n",
      "1 | pred_head | DeepLabV3PredictionHead | 16.1 M\n",
      "2 | loss_fn   | CrossEntropyLoss        | 0     \n",
      "------------------------------------------------------\n",
      "41.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "41.7 M    Total params\n",
      "166.736   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6/6 [00:01<00:00,  3.64it/s, v_num=114]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6/6 [00:02<00:00,  2.63it/s, v_num=114]\n",
      "Best model was saved at: /workspaces/2024-mo436-course-work/lightning_logs/version_114/checkpoints/epoch=1-step=12.ckpt\n",
      "Evaluating model with cap=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                    | Params\n",
      "------------------------------------------------------\n",
      "0 | backbone  | DeepLabV3Backbone       | 25.6 M\n",
      "1 | pred_head | DeepLabV3PredictionHead | 16.1 M\n",
      "2 | loss_fn   | CrossEntropyLoss        | 0     \n",
      "------------------------------------------------------\n",
      "41.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "41.7 M    Total params\n",
      "166.736   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 12/12 [00:02<00:00,  4.30it/s, v_num=115]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 12/12 [00:02<00:00,  4.29it/s, v_num=115]\n",
      "Best model was saved at: /workspaces/2024-mo436-course-work/lightning_logs/version_115/checkpoints/epoch=0-step=12.ckpt\n",
      "Evaluating model with cap=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                    | Params\n",
      "------------------------------------------------------\n",
      "0 | backbone  | DeepLabV3Backbone       | 25.6 M\n",
      "1 | pred_head | DeepLabV3PredictionHead | 16.1 M\n",
      "2 | loss_fn   | CrossEntropyLoss        | 0     \n",
      "------------------------------------------------------\n",
      "41.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "41.7 M    Total params\n",
      "166.736   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 24/24 [00:04<00:00,  4.88it/s, v_num=116]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 24/24 [00:05<00:00,  4.33it/s, v_num=116]\n",
      "Best model was saved at: /workspaces/2024-mo436-course-work/lightning_logs/version_116/checkpoints/epoch=1-step=48.ckpt\n",
      "Evaluating model with cap=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                    | Params\n",
      "------------------------------------------------------\n",
      "0 | backbone  | DeepLabV3Backbone       | 25.6 M\n",
      "1 | pred_head | DeepLabV3PredictionHead | 16.1 M\n",
      "2 | loss_fn   | CrossEntropyLoss        | 0     \n",
      "------------------------------------------------------\n",
      "41.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "41.7 M    Total params\n",
      "166.736   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 62/62 [00:11<00:00,  5.47it/s, v_num=117]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 62/62 [00:11<00:00,  5.19it/s, v_num=117]\n",
      "Best model was saved at: /workspaces/2024-mo436-course-work/lightning_logs/version_117/checkpoints/epoch=1-step=124.ckpt\n",
      "Evaluating model with cap=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                    | Params\n",
      "------------------------------------------------------\n",
      "0 | backbone  | DeepLabV3Backbone       | 25.6 M\n",
      "1 | pred_head | DeepLabV3PredictionHead | 16.1 M\n",
      "2 | loss_fn   | CrossEntropyLoss        | 0     \n",
      "------------------------------------------------------\n",
      "41.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "41.7 M    Total params\n",
      "166.736   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 124/124 [00:21<00:00,  5.70it/s, v_num=118]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 124/124 [00:22<00:00,  5.47it/s, v_num=118]\n",
      "Best model was saved at: /workspaces/2024-mo436-course-work/lightning_logs/version_118/checkpoints/epoch=1-step=248.ckpt\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "# Train all models\n",
    "best_checkpoint_path = {}\n",
    "for cap, d in dataloaders.items():\n",
    "    train_dl = d[\"train_dl\"]\n",
    "    val_dl = d[\"val_dl\"]\n",
    "    print(f\"Evaluating model with cap={cap}\")\n",
    "    downstream_model = build_downstream_model(pretrained_weights_filename)\n",
    "    checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"val_loss\")\n",
    "    trainer = L.Trainer(max_epochs=max_epochs, log_every_n_steps=1, callbacks=[checkpoint_callback])\n",
    "    trainer.fit(model=downstream_model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "    print(\"Best model was saved at:\", checkpoint_callback.best_model_path)\n",
    "    best_checkpoint_path[cap] = checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set output file names.\n",
    "- `output_file_prefix`: prefix to be used on all output files\n",
    "- `best_checkpoint_paths`: pickle file with dictionary mapping each `cap` value to the path to the file with the best model trained with the `cap` dataloader. \n",
    "- `results_filename`: text file with a summary of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrained_weights_filename:\n",
    "    output_file_prefix = \"pretrain-\" + \".\".join(pretrained_weights_filename.split(\".\")[:-1]) + \"-\"\n",
    "else:\n",
    "    output_file_prefix = \"pretrain-no-\"\n",
    "best_checkpoint_paths = output_file_prefix+'best_checkpoint_filenames.pkl'\n",
    "results_filename = output_file_prefix+f'results.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dictionary with paths to the best checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open(best_checkpoint_paths, 'wb') as f:\n",
    "    pickle.dump(best_checkpoint_path, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap = 0.01: /workspaces/2024-mo436-course-work/lightning_logs/version_113/checkpoints/epoch=1-step=2.ckpt\n",
      " - iou = 0.26956263184547424\n",
      "cap = 0.05: /workspaces/2024-mo436-course-work/lightning_logs/version_114/checkpoints/epoch=1-step=12.ckpt\n",
      " - iou = 0.322662353515625\n",
      "cap = 0.1: /workspaces/2024-mo436-course-work/lightning_logs/version_115/checkpoints/epoch=0-step=12.ckpt\n",
      " - iou = 0.2828516364097595\n",
      "cap = 0.2: /workspaces/2024-mo436-course-work/lightning_logs/version_116/checkpoints/epoch=1-step=48.ckpt\n",
      " - iou = 0.3802063465118408\n",
      "cap = 0.5: /workspaces/2024-mo436-course-work/lightning_logs/version_117/checkpoints/epoch=1-step=124.ckpt\n",
      " - iou = 0.4414859414100647\n",
      "cap = 1.0: /workspaces/2024-mo436-course-work/lightning_logs/version_118/checkpoints/epoch=1-step=248.ckpt\n",
      " - iou = 0.4527760148048401\n"
     ]
    }
   ],
   "source": [
    "import models.deeplabv3 as dlv3\n",
    "\n",
    "with open(best_checkpoint_paths, 'rb') as f:\n",
    "    best_checkpoint_path = pickle.load(f)\n",
    "\n",
    "cap_vs_iou = {}\n",
    "for cap, weights_filename in best_checkpoint_path.items():\n",
    "    print(f\"cap = {cap}: {weights_filename}\")\n",
    "    downstream_model = dlv3.DeepLabV3Model.load_from_checkpoint(weights_filename)\n",
    "    iou = evaluate_model(downstream_model, test_dl)\n",
    "    print(f\" - iou = {iou}\")\n",
    "    cap_vs_iou[cap] = iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write results summary to `results_filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cap (0.01): iou = 0.26956263184547424\n",
      "cap (0.05): iou = 0.322662353515625\n",
      "cap (0.1): iou = 0.2828516364097595\n",
      "cap (0.2): iou = 0.3802063465118408\n",
      "cap (0.5): iou = 0.4414859414100647\n",
      "cap (1.0): iou = 0.4527760148048401\n"
     ]
    }
   ],
   "source": [
    "with open(results_filename, 'w') as f:\n",
    "    for cap, iou in cap_vs_iou.items():\n",
    "        print(f\"cap ({cap}): iou = {iou}\")\n",
    "        f.write(f\"cap ({cap}): iou = {iou}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
